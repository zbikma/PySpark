{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55261,"status":"ok","timestamp":1724034925098,"user":{"displayName":"Zee Maz","userId":"16682775173566754666"},"user_tz":240},"id":"mi7wntzLe3I1","outputId":"06f5fe12-b9c4-4537-efb8-98ee01181994"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in ./myenv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.5.2)\n","Requirement already satisfied: findspark in ./myenv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.0.1)\n","Requirement already satisfied: requests in ./myenv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.32.3)\n","Requirement already satisfied: py4j==0.10.9.7 in ./myenv/lib/python3.9/site-packages (from pyspark->-r requirements.txt (line 1)) (0.10.9.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.9/site-packages (from requests->-r requirements.txt (line 3)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.9/site-packages (from requests->-r requirements.txt (line 3)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.9/site-packages (from requests->-r requirements.txt (line 3)) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.9/site-packages (from requests->-r requirements.txt (line 3)) (2024.7.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -r requirements.txt\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":135,"status":"ok","timestamp":1724037110715,"user":{"displayName":"Zee Maz","userId":"16682775173566754666"},"user_tz":240},"id":"kdRuhczinYlR"},"outputs":[],"source":["from dotenv import load_dotenv\n","load_dotenv()\n","\n","import os\n","\n","java_home = os.getenv(\"JAVA_HOME\")\n","spark_home = os.getenv(\"SPARK_HOME\")\n","os.environ[\"JAVA_HOME\"]=java_home\n","os.environ[\"SPARK_HOME\"] = spark_home\n","os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"SPARK_HOME\"], \"bin\")\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"executionInfo":{"elapsed":6944,"status":"ok","timestamp":1724037213149,"user":{"displayName":"Zee Maz","userId":"16682775173566754666"},"user_tz":240},"id":"vtCj7zy_nrYE","outputId":"8840b964-65cb-4038-83c9-c741256593a0"},"outputs":[{"ename":"Exception","evalue":"Unable to find py4j in /Users/LearningUser/my_local_spark/spark-3.5.2-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_python\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpy4j-*.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n","\u001b[0;31mIndexError\u001b[0m: list index out of range","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfindspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    163\u001b[0m                 spark_python\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n","\u001b[0;31mException\u001b[0m: Unable to find py4j in /Users/LearningUser/my_local_spark/spark-3.5.2-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly"]}],"source":["import findspark\n","findspark.init()\n","from pyspark import SparkContext"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/08/25 15:50:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]},{"data":{"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://192.168.2.119:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        "],"text/plain":["<SparkContext master=local[*] appName=pyspark-shell>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["cs = SparkContext.getOrCreate()\n","cs"]},{"cell_type":"markdown","metadata":{},"source":["#Download the dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n","  warnings.warn(\n"]},{"data":{"text/plain":["'/Users/LearningUser/Documents/Repos/PySpark'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import requests\n","os.getcwd()"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/LearningUser/Documents/Repos/PySpark/myenv/bin/python\n"]}],"source":["import sys\n","print(sys.executable)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m  \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrimes_2001_to_present.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent) \n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/urllib3/response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/urllib3/response.py:1209\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   1211\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m )\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/urllib3/response.py:1155\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_left\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:613\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n","File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n","File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import requests\n","url= \"https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\"\n","response =  requests.get(url)\n","with open(\"crimes_2001_to_present.csv\",\"wb\") as file:\n","    file.write(response.content) \n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/08/25 17:07:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------+-------------------+------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n","|      ID|Case Number|               Date|             Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|Latitude|Longitude|Location|\n","+--------+-----------+-------------------+------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n","| 5741943|   HN549294|2007-08-25 09:22:18|074XX N ROGERS AVE|0560|             ASSAULT|              SIMPLE|               OTHER| false|   false|2422|     024|  49|             1|     08A|        NULL|        NULL|2007|08/17/2015 03:03:...|    NULL|     NULL|    NULL|\n","| 1930689|   HH109118|2002-01-05 21:24:00|    007XX E 103 ST|0820|               THEFT|      $500 AND UNDER|         GAS STATION|  true|   false|0512|     005|NULL|          NULL|      06|        NULL|        NULL|2002|02/04/2016 06:33:...|    NULL|     NULL|    NULL|\n","|12416974|   JE293464|2011-08-10 00:01:00|   031XX W 53RD PL|1753|OFFENSE INVOLVING...|SEXUAL ASSAULT OF...|           RESIDENCE| false|    true|0923|     009|  14|            63|      02|        NULL|        NULL|2011|09/14/2023 03:41:...|    NULL|     NULL|    NULL|\n","|12536164|   JE439378|2015-09-24 00:00:00|   031XX W 53RD PL|1753|OFFENSE INVOLVING...|SEXUAL ASSAULT OF...|           APARTMENT| false|    true|0923|     009|  14|            63|      02|        NULL|        NULL|2015|09/14/2023 03:41:...|    NULL|     NULL|    NULL|\n","|12536166|   JE439332|2014-09-07 00:00:00|   031XX W 53RD PL|1753|OFFENSE INVOLVING...|SEXUAL ASSAULT OF...|           APARTMENT| false|    true|0923|     009|  14|            63|      02|        NULL|        NULL|2014|09/14/2023 03:41:...|    NULL|     NULL|    NULL|\n","+--------+-----------+-------------------+------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.sql.functions import to_timestamp,col,lit\n","df=spark.read.csv('crimes_2001_to_present.csv',header=True).withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a')).filter(col('Date') <= lit('2018-11-11'))\n","#orderedDf=df.orderBy(df[\"Date\"].desc())\n","df.show(5)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['Any', 'ArrayType', 'Callable', 'Column', 'DataFrame', 'DataType', 'Dict', 'Iterable', 'JVMView', 'List', 'Optional', 'PandasUDFType', 'PySparkTypeError', 'PySparkValueError', 'SparkContext', 'StringType', 'StructType', 'TYPE_CHECKING', 'Tuple', 'Type', 'Union', 'UserDefinedFunction', 'UserDefinedTableFunction', 'ValuesView', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_py_udf', '_create_py_udtf', '_from_numpy_type', '_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_columns', '_invoke_function_over_seq_of_columns', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aes_decrypt', 'aes_encrypt', 'aggregate', 'any_value', 'approxCountDistinct', 'approx_count_distinct', 'approx_percentile', 'array', 'array_agg', 'array_append', 'array_compact', 'array_contains', 'array_distinct', 'array_except', 'array_insert', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_prepend', 'array_remove', 'array_repeat', 'array_size', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bit_and', 'bit_count', 'bit_get', 'bit_length', 'bit_or', 'bit_xor', 'bitmap_bit_position', 'bitmap_bucket_number', 'bitmap_construct_agg', 'bitmap_count', 'bitmap_or_agg', 'bitwiseNOT', 'bitwise_not', 'bool_and', 'bool_or', 'broadcast', 'bround', 'btrim', 'bucket', 'call_function', 'call_udf', 'cardinality', 'cast', 'cbrt', 'ceil', 'ceiling', 'char', 'char_length', 'character_length', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'contains', 'conv', 'convert_timezone', 'corr', 'cos', 'cosh', 'cot', 'count', 'countDistinct', 'count_distinct', 'count_if', 'count_min_sketch', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'csc', 'cume_dist', 'curdate', 'current_catalog', 'current_database', 'current_date', 'current_schema', 'current_timestamp', 'current_timezone', 'current_user', 'date_add', 'date_diff', 'date_format', 'date_from_unix_date', 'date_part', 'date_sub', 'date_trunc', 'dateadd', 'datediff', 'datepart', 'day', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decimal', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'e', 'element_at', 'elt', 'encode', 'endswith', 'equal_null', 'every', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'extract', 'factorial', 'filter', 'find_in_set', 'first', 'first_value', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get', 'get_active_spark_context', 'get_json_object', 'getbit', 'greatest', 'grouping', 'grouping_id', 'has_numpy', 'hash', 'hex', 'histogram_numeric', 'hll_sketch_agg', 'hll_sketch_estimate', 'hll_union', 'hll_union_agg', 'hour', 'hours', 'hypot', 'ifnull', 'ilike', 'initcap', 'inline', 'inline_outer', 'input_file_block_length', 'input_file_block_start', 'input_file_name', 'inspect', 'instr', 'isnan', 'isnotnull', 'isnull', 'java_method', 'json_array_length', 'json_object_keys', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'last_value', 'lcase', 'lead', 'least', 'left', 'length', 'levenshtein', 'like', 'lit', 'ln', 'localtimestamp', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'make_date', 'make_dt_interval', 'make_interval', 'make_timestamp', 'make_timestamp_ltz', 'make_timestamp_ntz', 'make_ym_interval', 'map_concat', 'map_contains_key', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'mask', 'max', 'max_by', 'md5', 'mean', 'median', 'min', 'min_by', 'minute', 'mode', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'named_struct', 'nanvl', 'negate', 'negative', 'next_day', 'now', 'nth_value', 'ntile', 'nullif', 'nvl', 'nvl2', 'octet_length', 'overlay', 'overload', 'pandas_udf', 'parse_url', 'percent_rank', 'percentile', 'percentile_approx', 'pi', 'pmod', 'posexplode', 'posexplode_outer', 'position', 'positive', 'pow', 'power', 'printf', 'product', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'reduce', 'reflect', 'regexp', 'regexp_count', 'regexp_extract', 'regexp_extract_all', 'regexp_instr', 'regexp_like', 'regexp_replace', 'regexp_substr', 'regr_avgx', 'regr_avgy', 'regr_count', 'regr_intercept', 'regr_r2', 'regr_slope', 'regr_sxx', 'regr_sxy', 'regr_syy', 'repeat', 'replace', 'reverse', 'right', 'rint', 'rlike', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'sec', 'second', 'sentences', 'sequence', 'session_window', 'sha', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shiftleft', 'shiftright', 'shiftrightunsigned', 'shuffle', 'sign', 'signum', 'sin', 'sinh', 'size', 'skewness', 'slice', 'some', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'split_part', 'sqrt', 'stack', 'startswith', 'std', 'stddev', 'stddev_pop', 'stddev_samp', 'str_to_map', 'struct', 'substr', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sum_distinct', 'sys', 'tan', 'tanh', 'timestamp_micros', 'timestamp_millis', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_binary', 'to_char', 'to_csv', 'to_date', 'to_json', 'to_number', 'to_str', 'to_timestamp', 'to_timestamp_ltz', 'to_timestamp_ntz', 'to_unix_timestamp', 'to_utc_timestamp', 'to_varchar', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'try_add', 'try_aes_decrypt', 'try_avg', 'try_divide', 'try_element_at', 'try_multiply', 'try_remote_functions', 'try_subtract', 'try_sum', 'try_to_binary', 'try_to_number', 'try_to_timestamp', 'typeof', 'ucase', 'udf', 'udtf', 'unbase64', 'unhex', 'unix_date', 'unix_micros', 'unix_millis', 'unix_seconds', 'unix_timestamp', 'unwrap_udt', 'upper', 'url_decode', 'url_encode', 'user', 'var_pop', 'var_samp', 'variance', 'version', 'warnings', 'weekday', 'weekofyear', 'when', 'width_bucket', 'window', 'window_time', 'xpath', 'xpath_boolean', 'xpath_double', 'xpath_float', 'xpath_int', 'xpath_long', 'xpath_number', 'xpath_short', 'xpath_string', 'xxhash64', 'year', 'years', 'zip_with']\n"]}],"source":["from pyspark.sql import functions\n","print(dir(functions))"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on function substring in module pyspark.sql.functions:\n","\n","substring(str: 'ColumnOrName', pos: int, len: int) -> pyspark.sql.column.Column\n","    Substring starts at `pos` and is of length `len` when str is String type or\n","    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n","    when str is Binary type.\n","    \n","    .. versionadded:: 1.5.0\n","    \n","    .. versionchanged:: 3.4.0\n","        Supports Spark Connect.\n","    \n","    Notes\n","    -----\n","    The position is not zero based, but 1 based index.\n","    \n","    Parameters\n","    ----------\n","    str : :class:`~pyspark.sql.Column` or str\n","        target column to work on.\n","    pos : int\n","        starting position in str.\n","    len : int\n","        length of chars.\n","    \n","    Returns\n","    -------\n","    :class:`~pyspark.sql.Column`\n","        substring of given value.\n","    \n","    Examples\n","    --------\n","    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n","    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n","    [Row(s='ab')]\n","\n"]}],"source":["from pyspark.sql.functions import lower,upper,substring\n","help(substring)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- ID: string (nullable = true)\n"," |-- Case Number: string (nullable = true)\n"," |-- Date: timestamp (nullable = true)\n"," |-- Block: string (nullable = true)\n"," |-- IUCR: string (nullable = true)\n"," |-- Primary Type: string (nullable = true)\n"," |-- Description: string (nullable = true)\n"," |-- Location Description: string (nullable = true)\n"," |-- Arrest: string (nullable = true)\n"," |-- Domestic: string (nullable = true)\n"," |-- Beat: string (nullable = true)\n"," |-- District: string (nullable = true)\n"," |-- Ward: string (nullable = true)\n"," |-- Community Area: string (nullable = true)\n"," |-- FBI Code: string (nullable = true)\n"," |-- X Coordinate: string (nullable = true)\n"," |-- Y Coordinate: string (nullable = true)\n"," |-- Year: string (nullable = true)\n"," |-- Updated On: string (nullable = true)\n"," |-- Latitude: string (nullable = true)\n"," |-- Longitude: string (nullable = true)\n"," |-- Location: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+-----------------------------+\n","| lower(primary Type)| upper(Primary Type)|substring(Primary Type, 1, 4)|\n","+--------------------+--------------------+-----------------------------+\n","|             assault|             ASSAULT|                         ASSA|\n","|               theft|               THEFT|                         THEF|\n","|offense involving...|OFFENSE INVOLVING...|                         OFFE|\n","|offense involving...|OFFENSE INVOLVING...|                         OFFE|\n","|offense involving...|OFFENSE INVOLVING...|                         OFFE|\n","+--------------------+--------------------+-----------------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select(lower(col('primary Type')),upper(col('Primary Type')),substring(col('Primary Type'),1,4)).show(5)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["27.75% Crime resulted in Arrest \n"]},{"name":"stderr","output_type":"stream","text":["[Stage 106:=================================>                      (9 + 6) / 15]\r"]},{"name":"stdout","output_type":"stream","text":["top 3 locations reported most crimes are [Row(Location Description='STREET', count=1770643), Row(Location Description='RESIDENCE', count=1146481), Row(Location Description='APARTMENT', count=699355)]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.sql.functions import to_date,desc,asc,lower,upper\n","#df.filter(lower(col('Primary Type'))=='assault').groupby(to_date(col('Date')).alias('CrimeDate')).count().orderBy(asc('CrimeDate')).show(100)\n","arrestCrimes =  df.filter(lower(col('Arrest'))== 'true').count()\n","percentage = (arrestCrimes/df.select('Arrest').count()) * 100\n","print(f\"{percentage:.2f}% Crime resulted in Arrest \")\n","topLocations = df.groupby(col('Location Description')).count().orderBy('count',ascending=False).take(3)\n","print(f\"top 3 locations reported most crimes are {topLocations}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"j4_Lx82soBAy"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 2:==================================>                       (9 + 6) / 15]\r"]},{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+\n","|          min(Date)|          max(Date)|\n","+-------------------+-------------------+\n","|2001-01-01 00:00:00|2018-11-11 00:00:00|\n","+-------------------+-------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.sql.functions import min, max\n","recentDate = df.select(min(col('Date')),max(col('Date'))).show(1)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:==================================>                       (9 + 6) / 15]\r"]},{"name":"stdout","output_type":"stream","text":["+----------------------+----------------------+\n","|date_sub(min(Date), 3)|date_add(max(Date), 3)|\n","+----------------------+----------------------+\n","|            2000-12-29|            2018-11-14|\n","+----------------------+----------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.sql.functions import datepart,date_add,date_sub\n","df.select(date_sub(min(col('Date')),3),date_add(max(col('Date')),3)).show(1)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+-------------------+\n","|      Christmas Eve|          Christmas|         Boxing Day|\n","+-------------------+-------------------+-------------------+\n","|2019-12-24 13:30:00|2019-12-25 13:30:00|2019-12-26 13:30:00|\n","+-------------------+-------------------+-------------------+\n","\n"]}],"source":["from pyspark.sql.functions import to_date,to_timestamp,lit\n","df= spark.createDataFrame([('2019-12-24 13:30:00','2019-12-25 13:30:00','2019-12-26 13:30:00',)],['Christmas Eve','Christmas','Boxing Day'])\n","df.show(1)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------------+------------------------------------------------+\n","|to_date(Christmas, yyyy-MM-dd HH:mm:ss)|to_timestamp(Christmas Eve, yyyy-MM-dd HH:mm:ss)|\n","+---------------------------------------+------------------------------------------------+\n","|                             2019-12-25|                             2019-12-24 13:30:00|\n","+---------------------------------------+------------------------------------------------+\n","\n"]}],"source":["df.select(to_date(col('Christmas'),'yyyy-MM-dd HH:mm:ss'),to_timestamp('Christmas Eve','yyyy-MM-dd HH:mm:ss')).show(1)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|           christmas|\n","+--------------------+\n","|25/Dec/2019 13:30:00|\n","+--------------------+\n","\n"]}],"source":["df= spark.createDataFrame([('25/Dec/2019 13:30:00',)],['christmas'])\n","df.show(1)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------+---------------------------------------------+\n","|to_date(Christmas, dd/MMM/yyyy HH:mm:ss)|to_timestamp(Christmas, dd/MMM/yyyy HH:mm:ss)|\n","+----------------------------------------+---------------------------------------------+\n","|                              2019-12-25|                          2019-12-25 13:30:00|\n","+----------------------------------------+---------------------------------------------+\n","\n"]}],"source":["df.select(to_date(col('Christmas'),'dd/MMM/yyyy HH:mm:ss'),to_timestamp(col('Christmas'),'dd/MMM/yyyy HH:mm:ss')).show(1)\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------+\n","|Christmas             |\n","+----------------------+\n","|12/25/2019 01:30:00 PM|\n","+----------------------+\n","\n"]}],"source":["df=spark.createDataFrame([('12/25/2019 01:30:00 PM',)],['Christmas'])\n","df.show(1,truncate=False)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------+----------------------------------------------+\n","|to_date(Christmas, MM/dd/yyyy hh:mm:ss a)|to_timestamp(Christmas, MM/dd/yyyy hh:mm:ss a)|\n","+-----------------------------------------+----------------------------------------------+\n","|                               2019-12-25|                           2019-12-25 13:30:00|\n","+-----------------------------------------+----------------------------------------------+\n","\n"]}],"source":["df.select(to_date(col('Christmas'),'MM/dd/yyyy hh:mm:ss a'),to_timestamp(col('Christmas'),'MM/dd/yyyy hh:mm:ss a')).show(1)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------+----------------------+-------------------------+----+------------+-------------------------------------------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n","|ID      |Case Number|Date                  |Block                    |IUCR|Primary Type|Description                                            |Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|Updated On            |Latitude    |Longitude    |Location                     |\n","+--------+-----------+----------------------+-------------------------+----+------------+-------------------------------------------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n","|5741943 |HN549294   |08/25/2007 09:22:18 AM|074XX N ROGERS AVE       |0560|ASSAULT     |SIMPLE                                                 |OTHER               |false |false   |2422|024     |49  |1             |08A     |NULL        |NULL        |2007|08/17/2015 03:03:40 PM|NULL        |NULL         |NULL                         |\n","|25953   |JE240540   |05/24/2021 03:06:00 PM|020XX N LARAMIE AVE      |0110|HOMICIDE    |FIRST DEGREE MURDER                                    |STREET              |true  |false   |2515|025     |36  |19            |01A     |1141387     |1913179     |2021|11/18/2023 03:39:49 PM|41.917838056|-87.755968972|(41.917838056, -87.755968972)|\n","|26038   |JE279849   |06/26/2021 09:24:00 AM|062XX N MC CORMICK RD    |0110|HOMICIDE    |FIRST DEGREE MURDER                                    |PARKING LOT         |true  |false   |1711|017     |50  |13            |01A     |1152781     |1941458     |2021|11/18/2023 03:39:49 PM|41.995219444|-87.713354912|(41.995219444, -87.713354912)|\n","|13279676|JG507211   |11/09/2023 07:30:00 AM|019XX W BYRON ST         |0620|BURGLARY    |UNLAWFUL ENTRY                                         |APARTMENT           |false |false   |1922|019     |47  |5             |05      |1162518     |1925906     |2023|11/18/2023 03:39:49 PM|41.952345086|-87.677975059|(41.952345086, -87.677975059)|\n","|13274752|JG501049   |11/12/2023 07:59:00 AM|086XX S COTTAGE GROVE AVE|0454|BATTERY     |AGGRAVATED P.O. - HANDS, FISTS, FEET, NO / MINOR INJURY|SMALL RETAIL STORE  |true  |false   |0632|006     |6   |44            |08B     |1183071     |1847869     |2023|12/09/2023 03:41:24 PM|41.737750767|-87.604855911|(41.737750767, -87.604855911)|\n","+--------+-----------+----------------------+-------------------------+----+------------+-------------------------------------------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n","only showing top 5 rows\n","\n"]}],"source":["nrc= spark.read.csv('crimes_2001_to_present.csv',header=True)\n","nrc.show(5,truncate=False)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: list\n"]}],"source":["!list"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ISSUES.md                  crimes_2001_to_present.csv\n","README.md                  \u001b[34mmyenv\u001b[m\u001b[m\n","SETUP.md                   requirements.txt\n","SparkLearning.ipynb        settings.json\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+-------------+-------------------+-------+-----+-----+--------------------+------------+------------+------------+------------+------------+-----------+------------+--------------------+\n","|    DISTRICT|DISTRICT NAME|            ADDRESS|   CITY|STATE|  ZIP|             WEBSITE|       PHONE|         FAX|         TTY|X COORDINATE|Y COORDINATE|   LATITUDE|   LONGITUDE|            LOCATION|\n","+------------+-------------+-------------------+-------+-----+-----+--------------------+------------+------------+------------+------------+------------+-----------+------------+--------------------+\n","|Headquarters| Headquarters|3510 S Michigan Ave|Chicago|   IL|60653|http://home.chica...|        NULL|        NULL|        NULL| 1177731.401| 1881697.404|41.83070169|-87.62339535|(41.8307016873, -...|\n","|          18|   Near North| 1160 N Larrabee St|Chicago|   IL|60610|http://home.chica...|312-742-5870|312-742-5771|312-742-5773| 1172080.029| 1908086.527|41.90324165|-87.64335214|(41.9032416531, -...|\n","|          19|    Town Hall|   850 W Addison St|Chicago|   IL|60613|http://home.chica...|312-744-8320|312-744-4481|312-744-8011| 1169730.744| 1924160.317|41.94740046|-87.65151202|(41.9474004564, -...|\n","|          20|      Lincoln| 5400 N Lincoln Ave|Chicago|   IL|60625|http://home.chica...|312-742-8714|312-742-8803|312-742-8841| 1158399.146| 1935788.826|41.97954951|-87.69284451|(41.9795495131, -...|\n","|          22|  Morgan Park|1900 W Monterey Ave|Chicago|   IL|60643|http://home.chica...|312-745-0710|312-745-0814|312-745-0569| 1165825.476| 1830851.333|41.69143478|-87.66852039|(41.6914347795, -...|\n","+------------+-------------+-------------------+-------+-----+-----+--------------------+------------+------------+------------+------------+------------+-----------+------------+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["ps = spark.read.csv('PoliceStations.csv', header=True)\n","ps.show(5)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/08/25 17:08:13 WARN MemoryStore: Not enough space to cache rdd_32_7 in memory! (computed 50.9 MiB so far)\n","24/08/25 17:08:13 WARN BlockManager: Persisting block rdd_32_7 to disk instead.\n","24/08/25 17:08:13 WARN MemoryStore: Not enough space to cache rdd_32_4 in memory! (computed 51.1 MiB so far)\n","24/08/25 17:08:13 WARN BlockManager: Persisting block rdd_32_4 to disk instead.\n","24/08/25 17:08:13 WARN MemoryStore: Not enough space to cache rdd_32_5 in memory! (computed 51.1 MiB so far)\n","24/08/25 17:08:13 WARN BlockManager: Persisting block rdd_32_5 to disk instead.\n","24/08/25 17:08:13 WARN MemoryStore: Not enough space to cache rdd_32_6 in memory! (computed 51.0 MiB so far)\n","24/08/25 17:08:13 WARN BlockManager: Persisting block rdd_32_6 to disk instead.\n","24/08/25 17:08:21 WARN MemoryStore: Not enough space to cache rdd_32_5 in memory! (computed 51.1 MiB so far)\n","24/08/25 17:08:21 WARN MemoryStore: Not enough space to cache rdd_32_4 in memory! (computed 26.3 MiB so far)\n","24/08/25 17:08:21 WARN MemoryStore: Not enough space to cache rdd_32_6 in memory! (computed 51.0 MiB so far)\n","24/08/25 17:08:22 WARN MemoryStore: Not enough space to cache rdd_32_7 in memory! (computed 50.9 MiB so far)\n","24/08/25 17:08:24 WARN MemoryStore: Not enough space to cache rdd_32_9 in memory! (computed 26.3 MiB so far)\n","24/08/25 17:08:24 WARN BlockManager: Persisting block rdd_32_9 to disk instead.\n","24/08/25 17:08:25 WARN MemoryStore: Not enough space to cache rdd_32_10 in memory! (computed 26.3 MiB so far)\n","24/08/25 17:08:25 WARN BlockManager: Persisting block rdd_32_10 to disk instead.\n","24/08/25 17:08:25 WARN MemoryStore: Not enough space to cache rdd_32_11 in memory! (computed 26.4 MiB so far)\n","24/08/25 17:08:25 WARN BlockManager: Persisting block rdd_32_11 to disk instead.\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_9 in memory! (computed 1600.7 KiB so far)\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_10 in memory! (computed 1585.4 KiB so far)\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_11 in memory! (computed 1583.4 KiB so far)\n","24/08/25 17:08:32 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_32_7 in memory.\n","24/08/25 17:08:32 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_32_4 in memory.\n","24/08/25 17:08:32 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_32_6 in memory.\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_5 in memory! (computed 1579.4 KiB so far)\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_7 in memory! (computed 384.0 B so far)\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_6 in memory! (computed 384.0 B so far)\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_4 in memory! (computed 384.0 B so far)\n","24/08/25 17:08:32 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_32_10 in memory.\n","24/08/25 17:08:32 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_32_11 in memory.\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_9 in memory! (computed 1600.7 KiB so far)\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_10 in memory! (computed 384.0 B so far)\n","24/08/25 17:08:32 WARN MemoryStore: Not enough space to cache rdd_32_11 in memory! (computed 384.0 B so far)\n"]},{"name":"stdout","output_type":"stream","text":["[57.475s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 5.0 (TID 28): Retried waiting for GCLocker too often allocating 40897 words\n"]},{"name":"stderr","output_type":"stream","text":["24/08/25 17:08:32 ERROR Executor: Exception in task 9.0 in stage 5.0 (TID 28)\n","java.lang.OutOfMemoryError: Java heap space\n","\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n","\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2157)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n","\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n","\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n","\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n","\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n","\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n","\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:774)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","24/08/25 17:08:32 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 5.0 (TID 28),5,main]\n","java.lang.OutOfMemoryError: Java heap space\n","\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n","\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2157)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n","\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n","\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n","\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n","\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n","\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n","\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:774)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","24/08/25 17:08:32 WARN TaskSetManager: Lost task 9.0 in stage 5.0 (TID 28) (192.168.2.119 executor driver): java.lang.OutOfMemoryError: Java heap space\n","\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n","\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2157)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n","\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n","\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n","\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n","\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n","\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n","\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:774)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\n","24/08/25 17:08:32 ERROR TaskSetManager: Task 9 in stage 5.0 failed 1 times; aborting job\n","24/08/25 17:08:32 WARN TaskSetManager: Lost task 11.0 in stage 5.0 (TID 30) (192.168.2.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 5.0 failed 1 times, most recent failure: Lost task 9.0 in stage 5.0 (TID 28) (192.168.2.119 executor driver): java.lang.OutOfMemoryError: Java heap space\n","\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n","\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2157)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n","\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n","\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n","\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n","\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n","\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n","\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:774)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\n","Driver stacktrace:)\n","24/08/25 17:08:32 WARN TaskSetManager: Lost task 10.0 in stage 5.0 (TID 29) (192.168.2.119 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 5.0 failed 1 times, most recent failure: Lost task 9.0 in stage 5.0 (TID 28) (192.168.2.119 executor driver): java.lang.OutOfMemoryError: Java heap space\n","\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n","\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2157)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n","\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n","\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n","\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n","\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n","\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n","\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n","\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n","\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n","\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:774)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\n","Driver stacktrace:)\n","ERROR:root:Exception while sending command.\n","Traceback (most recent call last):\n","  File \"/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n","    return f(*a, **kw)\n","  File \"/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: <exception str() failed>\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n","    raise Py4JNetworkError(\"Answer from Java side is empty\")\n","py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","  File \"/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n","    raise Py4JNetworkError(\n","py4j.protocol.Py4JNetworkError: Error while sending or receiving\n","ERROR:root:Exception while sending command.\n","Traceback (most recent call last):\n","  File \"/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n","    raise Py4JNetworkError(\"Answer from Java side is empty\")\n","py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","  File \"/Users/LearningUser/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n","    raise Py4JNetworkError(\n","py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"]},{"ename":"Py4JError","evalue":"py4j does not exist in the JVM","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:132\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ParseException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Order matters. ParseException inherits AnalysisException.\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.AnalysisException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AnalysisException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.streaming.StreamingQueryException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n","File \u001b[0;32m~/Documents/Repos/PySpark/myenv/lib/python3.9/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n","\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM"]}],"source":["df.cache()\n","df.count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWo7sgQf5NCpDOWOUwrZe/","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
